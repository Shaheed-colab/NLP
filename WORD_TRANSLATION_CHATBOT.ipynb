{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "import joblib\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('urdu_to_roman_urdu_full_dataset.csv')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF]\", \"\", text)  # keep only Arabic/Urdu\n",
        "    return text.strip()\n",
        "\n",
        "df['Arabic'] = df['Arabic'].apply(clean_text)\n",
        "df['Urdu'] = df['Urdu'].apply(clean_text)\n",
        "\n",
        "df = df[df['Arabic'].str.len() > 2]\n",
        "\n",
        "nlp_ar = spacy.blank('ar')\n",
        "\n",
        "def ar_tokenization(text):\n",
        "    text = str(text).strip()\n",
        "    doc = nlp_ar(text)\n",
        "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "nlp_ur = spacy.blank('ur')\n",
        "\n",
        "def ur_tokenization(text):\n",
        "    text = str(text).strip()\n",
        "    doc = nlp_ur(text)\n",
        "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "nlp_ar = spacy.blank('en')\n",
        "\n",
        "def sr_tokenization(text):\n",
        "    text = str(text).strip()\n",
        "    doc = nlp_ar(text)\n",
        "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "df['Arabic'] = df['Arabic'].progress_apply(lambda x: ' '.join(ar_tokenization(x)))\n",
        "df['Urdu'] = df['Urdu'].progress_apply(lambda x: ' '.join(ur_tokenization(x)))\n",
        "\n",
        "df = df.dropna(subset=[\"Arabic\", \"Urdu\"])\n",
        "\n",
        "X_raw = df['Arabic'].astype(str).tolist()\n",
        "y = df['Urdu'].astype(str).tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
        "X = vectorizer.fit_transform(X_raw)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "test_words = ['ÿ£ŸäŸÜÿßŸÑŸÖÿ≥ÿ¨ÿØ']\n",
        "\n",
        "for word in test_words:\n",
        "    vec = vectorizer.transform([word])\n",
        "    urdu = clf.predict(vec)[0]\n",
        "    print(f\"Arabic: {word} ‚ûù Urdu: {urdu}\")\n",
        "\n",
        "joblib.dump(clf, 'arabic_to_urdu_model.pkl')\n",
        "joblib.dump(vectorizer, 'arabic_vectorizer.pkl')\n",
        "\n",
        "df['Urdu'] = df['Urdu'].astype(str).str.strip()\n",
        "df['Roman Urdu'] = df['Roman Urdu'].astype(str).str.strip()\n",
        "\n",
        "translit_dict = dict(zip(df['Urdu'], df['Roman Urdu']))\n",
        "\n",
        "def transliterate_urdu(word):\n",
        "    return translit_dict.get(word.strip(), \"ÿü\")\n",
        "\n",
        "test_words = ['ŸÖÿ≥ÿ¨ÿØ⁄©€Åÿß⁄∫€Å€í']\n",
        "\n",
        "for w in test_words:\n",
        "    print(f\"Urdu: {w} ‚ûù Roman Urdu: {transliterate_urdu(w)}\")\n",
        "\n",
        "with open(\"urdu_roman_translit.pkl\", \"wb\") as f:\n",
        "    pickle.dump(translit_dict, f)"
      ],
      "metadata": {
        "id": "dBBME2Wpuwgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transliterate_urdu(word):\n",
        "    return translit_dict.get(word.strip(), \"ÿü\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"üë§ You (Arabic word): \").strip()\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"ÿÆÿ±Ÿàÿ¨\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        vec = vectorizer.transform([user_input])\n",
        "        urdu_word = clf.predict(vec)[0]\n",
        "    except Exception as e:\n",
        "        urdu_word = \"ÿü\"\n",
        "        print(\"‚ö†Ô∏è Could not translate:\", e)\n",
        "\n",
        "    roman_word = transliterate_urdu(urdu_word)\n",
        "\n",
        "    print(\"üß† Urdu Translation:\", urdu_word)\n",
        "    print(\"üî§ Roman Transliteration:\", roman_word)\n"
      ],
      "metadata": {
        "id": "UV6nGg3GRaJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2LJhmg9SRo4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}